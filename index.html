
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Marigold-3DV</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="https://marigold-3dv.github.io/img/overview_combined.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1711">
    <meta property="og:image:height" content="576">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://marigold-3dv.github.io/"/>
    <meta property="og:title" content="Marigold: Is it all sunshine and Marigolds?" />
    <meta property="og:description" content="3D reconstruction methods such as Neural Radiance Fields (NeRFs) excel at rendering photorealistic novel views of complex scenes. However, recovering a high-quality NeRF typically requires tens to hundreds of input images, resulting in a time-consuming capture process. We present ReconFusion to reconstruct real-world scenes using only a few photos. Our approach leverages a diffusion prior for novel view synthesis, trained on synthetic and multiview datasets, which regularizes a NeRF-based 3D reconstruction pipeline at novel camera poses beyond those captured by the set of input images. Our method synthesizes realistic geometry and texture in underconstrained regions while preserving the appearance of observed regions. We perform an extensive evaluation across various real-world datasets, including forward-facing and 360-degree scenes, demonstrating significant performance improvements over previous few-view NeRF reconstruction approaches."/>

    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="ReconFusion: 3D Reconstruction with Diffusion Priors" />
    <meta name="twitter:description" content="3D reconstruction methods such as Neural Radiance Fields (NeRFs) excel at rendering photorealistic novel views of complex scenes. However, recovering a high-quality NeRF typically requires tens to hundreds of input images, resulting in a time-consuming capture process. We present ReconFusion to reconstruct real-world scenes using only a few photos. Our approach leverages a diffusion prior for novel view synthesis, trained on synthetic and multiview datasets, which regularizes a NeRF-based 3D reconstruction pipeline at novel camera poses beyond those captured by the set of input images. Our method synthesizes realistic geometry and texture in underconstrained regions while preserving the appearance of observed regions. We perform an extensive evaluation across various real-world datasets, including forward-facing and 360-degree scenes, demonstrating significant performance improvements over previous few-view NeRF reconstruction approaches."/>
    <meta name="twitter:image" content="https://reconfusion.github.io/img/overview_combined.png" />


<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ¤”</text></svg>">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">
	<link rel="stylesheet" href="css/fontawesome.all.min.css">
	<link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">


	<!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-8ZERS5BVPS"></script>
  <script>
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());

	gtag('config', 'G-8ZERS5BVPS');
  </script>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
	<script defer src="js/fontawesome.all.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/Chart.js/2.5.0/Chart.min.js"></script>

    <script src="js/app.js"></script>
    <script src="js/synced_video_selector.js"></script>

</head>

<body style="padding: 1%; width: 100%">
    <div class="container-lg text-center" style="max-width: 1500px; margin: auto;" id="main">
    <!-- <div class="container" id="main"> -->
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>Marigold</b>: Is it all sunshine and Marigolds?</br>
            </h2>
        </div>
        <div class="row text-center">
<div class="col-md-3">
    </div>
            <div class="col-md-6 text-center">
                <ul class="list-inline">
                    <li>
                            Moyang Li
                        <sup>1</sup>
                    </li>
                    <li>
                            Xiangyi Jia
                       <sup>1</sup>
                    </li>
                    <li>
                            Feng Tian
                        <sup>1</sup>
                    </li>
                    <li>
                            Yuxuan Xie
                        <sup>1</sup>
                    </li>
                    <wbr>
                </ul>
            </div>

<div class="col-md-3">
</div>
    <div class="col-md-12 text-center">
        <sup>1</sup>ETH Zurich
    </div>
    <br>

    <div class="row text-center">

                <span class="link-block">
            <!-- <a href="https://arxiv.org/abs/2312.02981"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="ai ai-arxiv"></i>
                </span>
                <span>Code</span>
            </a> -->
            <a href="https://github.com/MoyangLi00/MDS-NeRF"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="far fa-file-code"></i>
                </span>
                <span>Code</span>
            </a>
            </span>
    </div>



        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <video id="teaser-video-ours" width="100%" autoplay loop muted controls>
                  <source src="videos/h264_record3d_sheep.mp4" type="video/mp4" />
                </video> -->
<!--
                <video id="teaser-video-ours" width="100%" autoplay loop muted>
                  <source src="videos/teaser/grid_ours.mp4" type="video/mp4" />
                </video>
                <video id="teaser-video-zipenrf" width="100%" autoplay loop muted hidden>
                  <source src="videos/teaser/grid_zipnerf.mp4" type="video/mp4" />
                </video>

                <div class="switch-container-wrapper">
                    <div class="switch-container">
                        <span class="switch-label">Ours</span>
                        <label class="switch">
                            <input type="checkbox" id="teaserVideoSwitch" onclick="selectTeaserVideo()">
                            <div class="slider round"></div>
                        </label>
                        <span class="switch-label">Zip-NeRF</span>
                    </div>
                </div>
                <script>
                    function selectTeaserVideo() {
                      var video_ours = document.getElementById("teaser-video-ours");
                      var video_zipnerf = document.getElementById("teaser-video-zipenrf");
                      var videoSwitch = document.getElementById("teaserVideoSwitch");
                      if (videoSwitch.checked) {
                        video_zipnerf.hidden = false;
                        video_ours.hidden = true;
                      } else {
                        video_zipnerf.hidden = true;
                        video_ours.hidden = false;
                      }
                    }
                </script>
 -->
			<!-- </div>
        </div> -->


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    Marigold is a novel method of monocular depth estimation, which can predict affine-invariant depth from a single RGB image. In this 3DV project, we analyzed the performance of Marigold in various datasets, and evaluated the effectiveness of Marigold depth in downsream tasks, e.g. 3D reconstruction, pose estimation.
                    In terms of scene reconstruction, Neural Radiance Fields (NeRFs) recently showed superior performance in 3D reconstruction. However, NeRFs require a large number of input images to recover a high-quality 3D model. We evaluated the performance of Marigold in 3D reconstruction with sparse inputs. In terms of camera pose estimation, we implemented an effective optimization method based on FrozenRecon, to estimate the camera poses while aligning the outputs of Marigold to scale-consistent depths.
                </p>
            </div>
        </div>
        <br>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Method
                </h3>
                <h4>
                    Marigold Depth Evaluation
                </h4>

                <image src="img/overview_eval.png" width=60% style="display: block; margin: auto;"></image>
                <br>
                <p class="text-justify">
                    To evaluate Marigold performance, we run Marigold on various dataset with groundtruth depth, and align the affine-invariant depth from Marigold with the groundtruth by RANSAC. Then we compare the aligned depth and groundtruth depth qualitatively and quantitatively using absolute relative error.
                </p >
                <br>

                <h4>
                    3D Reconstruction with Marigold Depth
                </h4>
                <!-- <h3>
				  <b> <font color="#118ab2"></font><font color="#ef486e">Fusion</font></b> = <font color="#118ab2">3D Reconstruction</font>  + <font color="#ef486e">Diffusion Prior</font>
                </h3> -->
                <image src="img/overview_recon.png" width=60% style="display: block; margin: auto;"></image>
                <br>
                <p class="text-justify">
                    Marigold only predict affine-invariant depth, which could not be directly used for depth supervision. We need to align the scale and offset of Marigold depth with metric depth and use adjusted depth to supervise NeRF training. We jointly optimize the scale, offset and NeRF representation. We use nerfacto as our NeRF backbone due to its fast speed.
                </p>
                <br>

                <h4>
                    Camera Pose Estimation with Marigold
                </h4>
                <image src="img/overview_pose.png" width=60% style="display: block; margin: auto;"></image>
                <br>
                <p class="text-justify">
                    We implement an optimization pipeline (primarily based on FrozenRecon) to estimate camera poses. We jointly optimize the scale (global scale) and offset (global offset) for each image, as well as camera poses, camera focal lengths, and point weights used for local alignment. A local alignment module is utilized to guide the optimization of parameters, and accelerate convergence. The loss function consists of color and depth warping losses, along with a regularization term to enforce the point weights close to 1.
                </p>
            </div>
        </div><br>
	<!-- <div class="row">
		<div class="col-md-8 col-md-offset-2">
		<h3> ReconFusion enables high-quality 3D reconstruction from few views</h3><br>
                <video id="co3d-grid" width="100%" autoplay loop muted controls>
                  <source src="videos/results/co3d_3x5.mp4" type="video/mp4" />
                </video>
                <br>
                    <p class="text-justify" style="text-align: center;">ReconFusion generalizes to everyday scenes: the same diffusion model prior is used for <b>all</b> reconstruction results.</p>
			</div>
        </div> -->


        <br>
        <div class="row">
            <div class="col-md-8 col-md-offset-2" id="area1">
                <h3>
				  Results
                </h3>
                <h4>
                    3D Reconstruction with Marigold Depth
                </h4>

                <div class="text-center ">
                    <ul class="nav nav-pills center-pills">
                        <li class="method-pill active" data-value="fortress"
                            onclick="selectCompVideo(this, 'area1')"><a>Fortress</a></li>
                        <li class="method-pill" data-value="leaves"
                            onclick="selectCompVideo(this, 'area1')"><a>Leaves</a></li>
                        <li class="method-pill" data-value="orchids"
                            onclick="selectCompVideo(this, 'area1')"><a>Orchids</a></li>
                        <li class="method-pill" data-value="room"
                            onclick="selectCompVideo(this, 'area1')"><a>Room</a></li>
                    </ul>
                    <ul class="nav nav-pills center-pills">
                        <li class="method-pill" data-value="fern"
                        onclick="selectCompVideo(this, 'area1')"><a>Fern</a></li>
                        <li class="method-pill" data-value="flower"
                            onclick="selectCompVideo(this, 'area1')"><a>Flower</a></li>
                        <li class="method-pill" data-value="horns"
                            onclick="selectCompVideo(this, 'area1')"><a>Horns</a></li>
                        <li class="method-pill" data-value="trex"
                            onclick="selectCompVideo(this, 'area1')"><a>Trex</a></li>
                    </ul>
                </div>

                <!-- <script>
                    activeMethodPill = document.querySelector('.method-pill.active-pill');
                    activeScenePill = document.querySelector('.scene-pill.active-pill');
                    activeModePill = document.querySelector('.mode-pill.active-pill');
                </script> -->

                <div class="text-center">
                    <div class="video-container">
                        <video class="video" style="height: 280px; max-width: 100%;" id="compVideoArea1" loop playsinline autoplay muted>
                            <source id="sourceArea1" src="videos/h264_fortress.mp4" type="video/mp4" />
                        </video>
                    </div>
                    <p class="text-justify" style="text-align: center;">
                        Nerfacto (left) vs Ours (right) on LLFF dataset. Scene trained on <span id="compVideoValue">3</span> views. Try selecting scenes!
                    </p>
                    <!-- <script>
                        video0 = document.getElementById("compVideo0");
                        video0.addEventListener('loadedmetadata', function() {
                            if (activeVidID == 0 && select){
                                video0.play();
                                // print video size
                                console.log(video0.videoWidth, video0.videoHeight);
                                video0.hidden = false;
                                video1.hidden = true;
                            }
                        });
                    </script>

                    <script>
                        activeMethodPill = document.querySelector('.method-pill.active-pill'); -->
                    <!-- </script> -->
                    <br>
                </div>
            </div>

            <br>
            <!-- <div class="row"> -->
            <div class="col-md-8 col-md-offset-2" id="area2">

                <div class="text-center ">
                    <ul class="nav nav-pills center-pills">
                        <li class="method-pill active" data-value="record3d_sheep"
                            onclick="selectCompVideo(this, 'area2')"><a>Sheep</a></li>
                        <li class="method-pill" data-value="record3d_flower"
                            onclick="selectCompVideo(this, 'area2')"><a>Flower</a></li>
                    </ul>
                </div>

                    <!-- <script>
                        activeMethodPill = document.querySelector('.method-pill.active-pill');
                        activeScenePill = document.querySelector('.scene-pill.active-pill');
                        activeModePill = document.querySelector('.mode-pill.active-pill');
                    </script> -->

                <div class="text-center">
                    <div class="video-container">
                        <video class="video" style="height: 280px; max-width: 100%;" id="compVideoArea2" loop playsinline autoplay muted>
                            <source id="sourceArea2" src="videos/h264_record3d_sheep.mp4" type="video/mp4" />
                        </video>
                    </div>
                    <p class="text-justify" style="text-align: center; margin-bottom: 2em;">
                        Nerfacto (left) vs Ours (right) on Record3D dataset. Scene trained on <span id="compVideoValue">9</span> views. Try selecting scenes!
                    </p>
                        <!-- <script>
                            video0 = document.getElementById("compVideo0");
                            video0.addEventListener('loadedmetadata', function() {
                                if (activeVidID == 0 && select){
                                    video0.play();
                                    // print video size
                                    console.log(video0.videoWidth, video0.videoHeight);
                                    video0.hidden = false;
                                    video1.hidden = true;
                                }
                            });
                        </script> -->

                        <!-- <script>
                            activeMethodPill = document.querySelector('.method-pill.active-pill');
                        </script> -->
                </div>
            </div>
            <br>

            <div class="col-md-8 col-md-offset-2" id="area3">
                <h4>
                    Camera Pose Estimation with Marigold
                </h4>
<!--                 <image src="img/pose_est_table.png" width=95% style="display: block; margin: auto;"></image> -->
                <image src="img/frame10_result.png" width=100% style="display: block; margin: auto;"></image>
                <p class="text-justify" style="text-align: center;">
                    Colmap, FrozenRecon vs Our method on ScanNet dataset.
                </p>
                <p class="text-justify" style="text-align: center;font-size: 12px;">
                    Sample the first 10 frames from original datasets at intervals of 5 frames for test, as it takes too much time for Marigold and optimization to run on the entire dataset.
                </p>
                <br>

                <h4>
                    Limitation Analysis of Camera Pose Estimation
                </h4>
                <p class="text-justify" style="text-align: center;">
                    Here we show some failure cases on KITTI (nearly straight-line motion with small differences between frames) and ScanNet with large sampling interval of 100 frames.
               </p>
                <image src="img/fail_cases.png" width=95% style="display: block; margin: auto;"></image>
                <p class="text-justify" style="text-align: center;">
                    Our method and FrozenRecon might fail when the camera motion is in a nearly straight line, differences between frames are small, or frame interval is too large.
                </p>
            </div>
            <br>
        </div>
        <br>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Marigold Depth Evaluation
                </h3>
                <h4>
                    What Marigold is Good at
                </h4>
                <image src="img/good_at.png" width=60% style="display: block; margin: auto;"></image>
                <p class="text-justify">
                    Marigold is good at <b>estimating depth for objects with distinctive features</b>. Such as for The microphone in the red rectangle. Its depth even cannot be captured by sensor, but it can be estimated by Marigold. And Marigold can <b>prevent from being influenced by mirror</b>.
                </p>
                <br>

                <h4>
                    What Marigold is Not Good at: Edges, too Close and too Far Objects
                </h4>
                <image src="img/not_good_at.png" width=100% style="display: block; margin: auto;"></image>
                <br>

                <h4>
                    Quantitative Evaluation
                </h4>
                <image src="img/evaluate_result.png" width=60% style="display: block; margin: auto;"></image>
            </div>
        </div>

        <br>
        <br>

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
				  ReconFusion improves both few-view and many-view reconstruction
                </h3><br>
                <p class="text-justify" style="color:red">
                    Hover over the plot to show rendered video under different number of views.
                </p> -->

                <!-- top down layout -->
                <!-- <canvas id="sparsityChart" style="max-height: 300px; max-width: 500px; margin: auto;"></canvas>
                <script src="js/sparsity_chart.js"></script>
                <br>
                <p class="text-center">
                    Our diffusion prior improves performance over baseline Zip-NeRF in both the few-view and many-view sampling regimes.
                </p>

                <div class="text-center">
                    <div class="video-compare-container" id="materialsDiv">
                        <video class="video" id="sparsity" loop playsinline autoPlay muted src="videos/sparsity/stacked_vid.mp4" onplay="resizeAndPlay(this)"></video>
                        <canvas height=0 class="videoMerge" id="sparsityMerge"></canvas>
                    </div> -->
                    <!-- <canvas height=0 class="videoWrapper" id="sparsityVideoWrapper"></canvas> -->
			<!-- <em>Move the slider to adjust the number of views. The left column shows the nearest input view.</em>
                    <div class="slider-container" style="padding-left: 12%; padding-right: 11%;">
                        <input type="range" class="styled-slider" id="sparsitySlider" min="0" max="6" step="1" value="0" list="slider-labels">
                        <datalist id="slider-labels">
                            <option value="0">3</option>
                            <option value="1">6</option>
                            <option value="2">9</option>
                            <option value="3">18</option>
                            <option value="4">27</option>
                            <option value="5">54</option>
                            <option value="6">81</option>
                        </datalist>
                    </div>
                    <table style="text-align: left; padding-left: 0px; padding-right: 10%; width: 100%;">
                        <tr>
                            <th width="10%"></th>
                            <td width="10%">3</td>
                            <td width="10%">6</td>
                            <td width="10%">9</td>
                            <td width="10%">18</td>
                            <td width="10%">27</td>
                            <td width="10%">54</td>
                            <td width="10%">81</td>
                        </tr>
                    </table><br> -->
                    <!-- <div class="text-center">
                        Rendered video under <span id="sparsityValue" style="color: red;">3</span> views
                    </div> -->
                <!-- </div> -->

                <!-- left right layout -->
                <!-- <table style="width: 100%; border-collapse: collapse;">
                    <tr>
                      <td style="text-align: center;" >
                        <canvas id="sparsityChart" style="max-height: 250px; max-width: 200px; margin: auto;"></canvas>
                        <script src="js/sparsity_chart.js"></script>
                      </td>
                      <td style="text-align: center;">
                        <video class="video" width=100% id="sparsityVideo" loop playsinline autoplay muted onplay="playOnCanvas(this, 180)">
                            <source src="videos/sparsity/kitchenlego_3.mp4" type="video/mp4" />
                        </video>
                        <canvas height=0 class="videoWrapper" id="sparsityVideoWrapper"></canvas>
                      </td>
                    </tr>
                    <tr>
                      <td style="text-align: center;"></td>
                      <td style="text-align: center;">Rendered video under <span id="sparsityValue" style="color: red;">3</span> views</td>
                    </tr>
                </table> -->
            <!-- </div>
        </div> -->

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2 text-center">
                <h3>
                    ReconFusion distills a consistent 3D model from inconsistent samples
                </h3><br>

                <table style="margin-left: auto; margin-right: auto; width: 90%;">
                    <tr>
                      <th style="width: 4%;"></th>
                      <th style="text-align: center;width: 32%;">LLFF (3 views)</th>
                      <th style="text-align: center;width: 32%;">CO3D (6 views)</th>
					  <th style="text-align: center;width: 32%;">mip-NeRF 360 (9 views)</th>
                    </tr>
                    <tr>
                      <td style="text-align: center; writing-mode: vertical-lr; transform: rotate(180deg); width:4%">3D Reconstruction</td>
                      <td rowspan="2" colspan="3" style="width: 96%">
                        <video class="video" width=100% loop playsinline autoplay muted controls>
                            <source src="videos/ablation/recon_vs_samples.mp4" />
                        </video>
                      </td> -->
                      <!-- <td style="text-align: center;">
                        <video class="video" width=100% loop playsinline autoplay muted style="max-height: 150px;">
                            <source src="videos/ablation/z123_nerf.mp4" />
                        </video>
                      </td>
                      <td style="text-align: center;">
                        <video class="video" width=100% loop playsinline autoplay muted style="max-height: 150px;">
                            <source src="videos/ablation/scratch_nerf.mp4" />
                        </video>
                      </td>
                      <td style="text-align: center;">
                        <video class="video" width=100% loop playsinline autoplay muted style="max-height: 150px;">
                            <source src="videos/ablation/ours_nerf.mp4" />
                        </video>
                      </td> -->
                    <!-- </tr>
                    <tr>
					  <td style="text-align: center; writing-mode: vertical-lr; transform: rotate(180deg); width:4%">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Samples&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td><td></td> -->
                      <!-- <td style="text-align: center;">
                        <video class="video" width=100% loop playsinline autoplay muted style="max-height: 150px;">
                            <source src="videos/ablation/z123_samples.mp4" />
                        </video>
                      </td>
                      <td style="text-align: center;">
                        <video class="video" width=100% loop playsinline autoplay muted style="max-height: 150px;">
                            <source src="videos/ablation/scratch_samples.mp4" />
                        </video>
                      </td>
                      <td style="text-align: center;">
                        <video class="video" width=100% loop playsinline autoplay muted style="max-height: 150px;">
                            <source src="videos/ablation/ours_samples.mp4" />
                        </video>
                      </td> -->
                    <!-- </tr>
                </table>
                <br> -->
<!--
                <p class="text-justify" style="width: 85%; margin: auto;"> -->
                <!-- <p class="text-justify" style="width:85%; margin:auto">
                    ReconFusion recovers consistent 3D reconstructions (top) from a diffusion model that produces image samples independently for each viewpoint (bottom). These samples are not multiview consistent, but can produce high-quality 3D reconstructions when used as a prior in optimization.
                </p>
            </div>
        </div>
        <br>
        <br> -->

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
				  <p class="text-justify">
                    <textarea id="bibtex" class="form-control" readonly>
@article{wu2023reconfusion,
    title={ReconFusion: 3D Reconstruction with Diffusion Priors},
    author={Rundi Wu and Ben Mildenhall and Philipp Henzler and
			Keunhong Park and Ruiqi Gao and Daniel Watson and
			Pratul P. Srinivasan and Dor Verbin and Jonathan T. Barron
			and Ben Poole and Aleksander Holynski},
    journal={arXiv},
    year={2023}
	}</textarea></p>
                </div>
            </div>
        </div> -->

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                We would like to thank our supervisors Mihai Dusmanu and Zuria Bauer for their valuable guidance in this 3DV project.
                    <br><br>
                The website template was borrowed from <a href="http://mgharbi.com/">MichaÃ«l Gharbi</a> and <a href="https://dorverbin.github.io/refnerf">Ref-NeRF</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
